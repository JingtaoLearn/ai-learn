# Open Claw

Self-hosted AI agent framework. Runs directly on the VM host (not in Docker).

## Architecture

Open Claw uses two model providers and two systemd user services:

### Model Providers

1. **openai-codex** (primary) — Direct connection to OpenAI API via OAuth (ChatGPT Plus account). Provides GPT-5.3 Codex as the default agent model.
2. **maestro** (fallback) — Routes through a local OpenAI-compatible proxy to an upstream LLM API (`S_LLM_API_URL`), providing access to Claude models via GitHub Copilot.

### Services

1. **openclaw-proxy** (port 19999) — OpenAI-compatible reverse proxy that flattens multimodal content arrays into plain strings before forwarding to the upstream LLM API. Only used by the maestro provider.
2. **openclaw-gateway** (port 18789) — The main Open Claw gateway that serves agent requests.

```
                    ┌─→ OpenAI API (openai-codex, OAuth)  ← PRIMARY
OpenClaw Gateway ──┤
                    └─→ Proxy (:19999) → LLM API (maestro) ← FALLBACK
```

### Model Routing

| Model | Provider | Auth | Role |
|-------|----------|------|------|
| `openai-codex/gpt-5.3-codex` | OpenAI API | OAuth (ChatGPT Plus) | Primary |
| `maestro/claude-opus-4.6` | Upstream LLM API | API Key | Fallback 1 |
| `maestro/claude-sonnet-4.5` | Upstream LLM API | API Key | Fallback 2 |
| `maestro/claude-haiku-4.5` | Upstream LLM API | API Key | Available |

## Prerequisites

- Node.js (via nvm, fnm, or system package)
- npm with a global prefix configured (`~/.npm-global`)

## Installation

```bash
npm install -g openclaw
```

## Configuration

### Maximum Permissions Configuration

The `openclaw.json` file is configured with **maximum permissions** for full functionality. This includes:

**Tools Enabled:**
- `exec`, `process` - Command execution and process management
- `read`, `write`, `edit`, `apply_patch` - File system operations
- `browser` - Browser control (requires separate setup)
- `web`, `web_fetch`, `web_search` - Web access and search
- `memory` - Memory and embedding search
- `cron` - Scheduled task execution

**Security Features:**
- `sandbox.mode: "off"` - No sandboxing for maximum flexibility
- `elevated.enabled: true` - Allows privileged command execution
- `gateway.bind: "loopback"` - Restricts gateway to localhost only (recommended)
- `logging.redactSensitive: "tools"` - Redacts sensitive data in logs
- `discovery.mdns.mode: "minimal"` - Limits information disclosure

⚠️ **Security Warning:** This configuration grants extensive system access. Only use in trusted environments.

### Environment Variables

The following environment variables must be set (e.g., in `/etc/environment` via `vm/scripts/03-set-env.sh`):

| Variable | Description |
|---|---|
| `S_LLM_API_URL` | Upstream LLM API URL for maestro provider (set via `vm/scripts/03-set-env.sh --llm-api-url`) |
| `OPENCLAW_API_KEY` | API key for the maestro upstream model provider |
| `OPENCLAW_GATEWAY_TOKEN` | Authentication token for gateway access (used in `openclaw.json`) |

**Note:** The `openai-codex` provider uses OAuth authentication (ChatGPT Plus account) instead of API keys. Run `openclaw configure --section model` to complete OAuth login.

### Symlinks

Configuration files in this repo are symlinked to their system locations so edits in either place stay in sync:

| Repo File | System Location |
|---|---|
| `openclaw.json` | `~/.openclaw/openclaw.json` |
| `openai-compat-proxy.js` | `~/.openclaw/openai-compat-proxy.js` |

**Note:** Both systemd service files (`.example`) are stored here for reference only. `openclaw-gateway.service` is auto-generated by `openclaw gateway install`. `openclaw-proxy.service` should be copied and customized before deploying.

### Setup

Run the setup script to create symlinks and enable services:

```bash
./setup.sh
```

The script will:
- Validate that required environment variables are set
- Back up existing files before replacing with symlinks
- Create symlinks from system locations to repo files
- Reload systemd and enable/start both services

## Testing Model Integration

### OpenAI Codex (Primary)

After OAuth login, verify the primary provider:

```bash
# Check auth status and quota
openclaw channels list

# List available models
openclaw models list --all | grep "openai-codex"
```

### Maestro Fallback

To verify that OpenClaw can access Claude models through the maestro proxy:

```bash
# Test the proxy directly
curl -s -X POST http://127.0.0.1:19999/api/openai/v1/chat/completions \
  -H "Authorization: Bearer ${OPENCLAW_API_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"model":"claude-opus-4.6","messages":[{"role":"user","content":"Hello"}],"max_tokens":50}' \
  | jq -r '.choices[0].message.content'

# List maestro models
openclaw models list --all | grep "maestro/"
```

### Available Models

| Model | Provider | Context | Role |
|-------|----------|---------|------|
| `openai-codex/gpt-5.3-codex` | OpenAI API (OAuth) | 200k | Primary |
| `maestro/claude-opus-4.6` | Upstream LLM API | 200k | Fallback |
| `maestro/claude-sonnet-4.5` | Upstream LLM API | 200k | Fallback |
| `maestro/claude-haiku-4.5` | Upstream LLM API | 200k | Available |

## Useful Commands

```bash
# Check service status
systemctl --user status openclaw-gateway openclaw-proxy

# View logs
journalctl --user -u openclaw-gateway -f
journalctl --user -u openclaw-proxy -f
tail -f /tmp/openclaw/openclaw-$(date +%Y-%m-%d).log

# Restart services
systemctl --user restart openclaw-proxy
systemctl --user restart openclaw-gateway

# Update openclaw
npm update -g openclaw
openclaw gateway install   # regenerates gateway service file

# Enable lingering (services persist after logout)
sudo loginctl enable-linger "$USER"

# Security validation
openclaw doctor --fix              # validate and fix configuration issues
openclaw security audit --deep     # deep security audit
chmod 700 ~/.openclaw              # secure directory permissions
chmod 600 ~/.openclaw/openclaw.json # secure config file

# Channel management
openclaw channels status
openclaw channels list
openclaw status --deep

# Test sending WhatsApp message
openclaw message send --channel whatsapp --target +1234567890 --message "Hello"
```

## WhatsApp Channel

### Setup

1. Enable the WhatsApp plugin:
```bash
openclaw plugins enable whatsapp
systemctl --user restart openclaw-gateway
```

2. Add WhatsApp channel:
```bash
openclaw channels add --channel whatsapp --name "WhatsApp"
```

3. Link your WhatsApp account (scan QR code with your phone):
```bash
openclaw channels login --channel whatsapp --verbose
```

### Troubleshooting

**Issue**: Not receiving messages

The WhatsApp channel uses a "pairing" DM policy by default. Messages from unknown senders will be blocked. To change this:

1. Check current config:
```bash
openclaw config get channels.whatsapp.accounts.default.dmPolicy
```

2. Allow all DMs (open mode - **not recommended for production**):
```bash
openclaw config set channels.whatsapp.accounts.default.dmPolicy open
systemctl --user restart openclaw-gateway
```

3. Or manage allowlist:
```bash
# List pairing requests
openclaw pairing list whatsapp

# Approve a sender
openclaw pairing approve whatsapp <code>
```

**Issue**: Connection keeps dropping

If you see "Connection Terminated" errors in logs, restart the gateway:
```bash
systemctl --user restart openclaw-gateway
# Wait a few seconds for WhatsApp to reconnect
sleep 5
openclaw status
```

## File Overview

```
open-claw/
├── README.md
├── setup.sh                               # Symlink + service setup
├── openclaw.json                          # Gateway config (symlinked)
├── openai-compat-proxy.js                 # Proxy script (symlinked)
└── systemd/
    ├── openclaw-gateway.service.example   # Reference only (auto-generated)
    └── openclaw-proxy.service.example    # Proxy unit file template
```
